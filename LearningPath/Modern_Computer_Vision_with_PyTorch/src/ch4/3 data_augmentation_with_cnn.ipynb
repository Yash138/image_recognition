{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from torchvision import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_folder = '../../data/FMNIST/'\n",
    "fmnist = datasets.FashionMNIST(data_folder, train=True, download=True)\n",
    "tr_images = fmnist.data\n",
    "tr_targets = fmnist.targets\n",
    "\n",
    "val_fmnist = datasets.FashionMNIST(data_folder, train=False, download=True)\n",
    "val_images = val_fmnist.data\n",
    "val_targets = val_fmnist.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important parameters in the `Affine` method\n",
    "* **scale** specifies the amount of zoom that is to be done for the image\n",
    "* **translate_percent** specifies the amount of translation as a percentage of the image's height and width\n",
    "* **translate_px** : specifies the amount of translation as an absolute number of pixels\n",
    "* **rotate** : specifies the amount of rotation that is to be done on the image\n",
    "* **shear** : specifies the amount of rotation that is to be done on part of the image\n",
    "* **mode** : there are different modes we can use to fill the values of newly created pixels\n",
    "    * **constant** : Pads with a constant value.\n",
    "    * **edge** : Pads with the edge values of the array.\n",
    "    * **symmetric** : Pads with the reflection of the vector mirrored along the edge of the array.\n",
    "    * **reflect** : Pads with the reflection of the vector mirrored on the first and last values of the vector along each axis.\n",
    "    * **wrap** : Pads with the wrap of the vector along the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = iaa.Sequential([\n",
    "    iaa.Affine(translate_px={'x':(-10,10)},\n",
    "               mode='constant')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMNISTDataset(Dataset):\n",
    "    def __init__(self, x, y, aug=None) -> None:\n",
    "        super().__init__()\n",
    "        self.x, self.y = x, y\n",
    "        self.aug = aug\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # logic to modify a batch of images\n",
    "        # print(len(batch), batch)\n",
    "        ims, classes = list(zip(*batch))\n",
    "        # print(classes)\n",
    "        # ims = batch\n",
    "        if self.aug:\n",
    "            ims1 = ims.cpu().numpy().tolist() if 'torch' in str(type(ims)) else ims\n",
    "            ims = self.aug.augment_images(images=ims1)\n",
    "        ims = torch.tensor(ims)[:, None, :, :].to(device)/255\n",
    "        classes = torch.tensor(classes).to(device)\n",
    "        return ims, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = FMNISTDataset(tr_images, tr_targets, aug=aug)\n",
    "    'notice the collate_fn argument'\n",
    "    trn_dl = DataLoader(train, batch_size=64,\n",
    "                collate_fn=train.collate_fn, shuffle=True)\n",
    "    val = FMNISTDataset(val_images, val_targets)\n",
    "    val_dl = DataLoader(val, batch_size=len(val_images),\n",
    "                collate_fn=val.collate_fn, shuffle=True)\n",
    "    return trn_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = FMNISTDataset(tr_images, tr_targets, aug=aug)\n",
    "# train.collate_fn(torch.tensor(tr_images[:32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ..., 180,   0,   0],\n",
       "        [  0,   0,   0, ...,  72,   0,   0],\n",
       "        [  0,   0,   0, ...,  70,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,  39,   1,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ..., 238,   0,   0],\n",
       "        [  0,   0,   0, ..., 131,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "       [[  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0],\n",
       "        [  0,   0,   0, ...,   7,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   0,   9,   0],\n",
       "        [  0,   0,   0, ...,   0,   3,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_images.cpu().numpy()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, kernel_size=3),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, kernel_size=3),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(3200, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, model, loss_fn, optim):\n",
    "    model.train()\n",
    "    prediction = model(x)\n",
    "    loss_val = loss_fn(prediction, y)\n",
    "    loss_val.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    return loss_val.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'deepcopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ix, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m(trn_dl)):\n\u001b[0;32m      7\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(x), \u001b[38;5;28mtype\u001b[39m(y))\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[80], line 21\u001b[0m, in \u001b[0;36mFMNISTDataset.collate_fn\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug:\n\u001b[0;32m     20\u001b[0m     ims1 \u001b[38;5;241m=\u001b[39m ims\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(ims)) \u001b[38;5;28;01melse\u001b[39;00m ims\n\u001b[1;32m---> 21\u001b[0m     ims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug\u001b[38;5;241m.\u001b[39maugment_images(images\u001b[38;5;241m=\u001b[39mims1)\n\u001b[0;32m     22\u001b[0m ims \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ims)[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m     23\u001b[0m classes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(classes)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\imgaug\\augmenters\\meta.py:822\u001b[0m, in \u001b[0;36mAugmenter.augment_images\u001b[1;34m(self, images, parents, hooks)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Augment a batch of images.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m \n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    821\u001b[0m iabase\u001b[38;5;241m.\u001b[39m_warn_on_suspicious_multi_image_shapes(images)\n\u001b[1;32m--> 822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment_batch_(\n\u001b[0;32m    823\u001b[0m     UnnormalizedBatch(images\u001b[38;5;241m=\u001b[39mimages),\n\u001b[0;32m    824\u001b[0m     parents\u001b[38;5;241m=\u001b[39mparents,\n\u001b[0;32m    825\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks\n\u001b[0;32m    826\u001b[0m )\u001b[38;5;241m.\u001b[39mimages_aug\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\imgaug\\augmenters\\meta.py:597\u001b[0m, in \u001b[0;36mAugmenter.augment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    595\u001b[0m     batch_unnorm \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    596\u001b[0m     batch_norm \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto_normalized_batch()\n\u001b[1;32m--> 597\u001b[0m     batch_inaug \u001b[38;5;241m=\u001b[39m batch_norm\u001b[38;5;241m.\u001b[39mto_batch_in_augmentation()\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, Batch):\n\u001b[0;32m    599\u001b[0m     batch_norm \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\imgaug\\augmentables\\batches.py:451\u001b[0m, in \u001b[0;36mBatch.to_batch_in_augmentation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mcopy_augmentables(var)\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m var\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _BatchInAugmentation(\n\u001b[1;32m--> 451\u001b[0m     images\u001b[38;5;241m=\u001b[39m_copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_unaug),\n\u001b[0;32m    452\u001b[0m     heatmaps\u001b[38;5;241m=\u001b[39m_copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheatmaps_unaug),\n\u001b[0;32m    453\u001b[0m     segmentation_maps\u001b[38;5;241m=\u001b[39m_copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_maps_unaug),\n\u001b[0;32m    454\u001b[0m     keypoints\u001b[38;5;241m=\u001b[39m_copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeypoints_unaug),\n\u001b[0;32m    455\u001b[0m     bounding_boxes\u001b[38;5;241m=\u001b[39m_copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbounding_boxes_unaug),\n\u001b[0;32m    456\u001b[0m     polygons\u001b[38;5;241m=\u001b[39m_copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolygons_unaug),\n\u001b[0;32m    457\u001b[0m     line_strings\u001b[38;5;241m=\u001b[39m_copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_strings_unaug)\n\u001b[0;32m    458\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\imgaug\\augmentables\\batches.py:447\u001b[0m, in \u001b[0;36mBatch.to_batch_in_augmentation.<locals>._copy\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy\u001b[39m(var):\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# TODO first check here if _aug is set and if it is then use that?\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mcopy_augmentables(var)\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m var\n",
      "File \u001b[1;32mc:\\Users\\yashl\\anaconda3\\Lib\\site-packages\\imgaug\\augmentables\\utils.py:19\u001b[0m, in \u001b[0;36mcopy_augmentables\u001b[1;34m(augmentables)\u001b[0m\n\u001b[0;32m     17\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mcopy(augmentable))\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(augmentable\u001b[38;5;241m.\u001b[39mdeepcopy())\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'deepcopy'"
     ]
    }
   ],
   "source": [
    "trn_dl, val_dl = get_data()\n",
    "model, loss_fn, optimizer = get_model()\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    for ix, batch in enumerate(iter(trn_dl)):\n",
    "        x, y = batch\n",
    "        print(type(x), type(y))\n",
    "        batch_loss = train_batch(x, y, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
